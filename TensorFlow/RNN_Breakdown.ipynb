{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to RNNs\n",
    "\n",
    "RNNs have the unique quality of having \"memory\" of previous data, making them especially useful in temporal or sequential data of some kind (e.g., weather forcasting, sales forecasting, stock market predictions, etc.).\n",
    "\n",
    "Other applications of RNNs are in the field of text analysis and text generation. Textual data is inherently sequential and RNNs can be used to classify parts of speech and even generate their own text, such as with autocorrect or autocomplete functionality.\n",
    "\n",
    "The following three built-in RNN layers available in Keras will be reviewed:\n",
    "\n",
    "1. `SimpleRNN`\n",
    "2. `GRU` (Gated Recurrent Unit)\n",
    "3. `LSTM` (Long Short-Term Memory)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the required libraries.\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate a Sequential model\n",
    "model = tf.keras.Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 10)                70        \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 2)                 22        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 92\n",
      "Trainable params: 92\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Add a hidden layer with 10 nodes and an input shape of 6 nodes.\n",
    "model.add(layers.Dense(10, input_shape=(6,)))\n",
    "\n",
    "# Add an output layer with 2 nodes.\n",
    "model.add(layers.Dense(2))\n",
    "\n",
    "# confirming that model has total params=92.\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_2 (Dense)             (None, 10)                70        \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 10)                110       \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 10)                110       \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 2)                 22        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 312\n",
      "Trainable params: 312\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.Sequential()\n",
    "\n",
    "# input layer\n",
    "model.add(layers.Dense(10, input_shape=(6,), activation='relu'))\n",
    "\n",
    "#adding two more 10 node hidden layers\n",
    "model.add(layers.Dense(10, activation='relu'))\n",
    "model.add(layers.Dense(10, activation='relu'))\n",
    "\n",
    "#output layer\n",
    "model.add(layers.Dense(2))\n",
    "\n",
    "# compile\n",
    "model.compile(optimizer='Adam', loss='mean_absolute_error')\n",
    "\n",
    "# produce summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a Reccurrent Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " simple_rnn (SimpleRNN)      (None, 10)                170       \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 10)                110       \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 2)                 22        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 302\n",
      "Trainable params: 302\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# instantiating Sequential model\n",
    "model = tf.keras.Sequential()\n",
    "\n",
    "# SimpleRNN layer to the model; 4 steps of memory, 6 features/timestop, and 10 nodes in the first hidden layer\n",
    "model.add(layers.SimpleRNN(10, input_shape=(4, 6), activation='relu'))\n",
    "\n",
    "# adding second hidden layer with 10 nodes\n",
    "model.add(layers.Dense(10, activation='relu'))\n",
    "\n",
    "# output with two nodes\n",
    "model.add(layers.Dense(2))\n",
    "\n",
    "#compiling model\n",
    "model.compile(optimizer='sgd', loss='mean_squared_error')\n",
    "\n",
    "# model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 10)                680       \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 10)                110       \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 2)                 22        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 812\n",
      "Trainable params: 812\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.Sequential()\n",
    "\n",
    "# input layer\n",
    "model.add(layers.LSTM(10, input_shape=(4, 6), activation='relu'))\n",
    "\n",
    "# hidden layer\n",
    "model.add(layers.Dense(10, activation='relu'))\n",
    "\n",
    "#output layer\n",
    "model.add(layers.Dense(2))\n",
    "\n",
    "# compile model\n",
    "model.compile(optimizer='Adam', loss='mean_squared_error')\n",
    "\n",
    "# summarize model\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " time_distributed_6 (TimeDis  (None, 8, 11, 8, 64)     320       \n",
      " tributed)                                                       \n",
      "                                                                 \n",
      " time_distributed_7 (TimeDis  (None, 8, 5, 4, 64)      0         \n",
      " tributed)                                                       \n",
      "                                                                 \n",
      " time_distributed_8 (TimeDis  (None, 8, 1280)          0         \n",
      " tributed)                                                       \n",
      "                                                                 \n",
      " lstm_3 (LSTM)               (None, 10)                51640     \n",
      "                                                                 \n",
      " dense_14 (Dense)            (None, 10)                110       \n",
      "                                                                 \n",
      " dense_15 (Dense)            (None, 2)                 22        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 52,092\n",
      "Trainable params: 52,092\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.Sequential()\n",
    "\n",
    "model.add(layers.TimeDistributed(layers.Conv2D(64, 2, activation='relu'), input_shape=(8,12,9,1)))\n",
    "model.add(layers.TimeDistributed(layers.MaxPooling2D()))\n",
    "model.add(layers.TimeDistributed(layers.Flatten()))\n",
    "\n",
    "# LSTM layer\n",
    "model.add(layers.LSTM(10, activation='relu'))\n",
    "\n",
    "# hidden layer\n",
    "model.add(layers.Dense(10, activation='relu'))\n",
    "\n",
    "#output layer\n",
    "model.add(layers.Dense(2))\n",
    "# compile model\n",
    "model.compile(optimizer='Adam', loss='mean_squared_loss')\n",
    "\n",
    "# model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bidirectional Layer\n",
    "\n",
    "A bidirectional layers is a feature that we can add to any recurrent layer as a wrapper. \n",
    "\n",
    "TensorFlow conveniently includes a `Bidirectional` wrapper that can be utilized with any recurrent layer, like `SimpleRNN` or `LSTM`. This wrapper enables the layer to retain \"memory\" in both the forward and reverse directions of the sequence. To implement this wrapper, it is applied to an RNN layer in a manner akin to the application of the `TimeDistributed` wrapper on `Con`v layers previously discussed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " bidirectional_1 (Bidirectio  (None, 20)               1200      \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " dense_18 (Dense)            (None, 10)                210       \n",
      "                                                                 \n",
      " dense_19 (Dense)            (None, 2)                 22        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,432\n",
      "Trainable params: 1,432\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# instantiating a model\n",
    "model = tf.keras.Sequential()\n",
    "\n",
    "# input layer\n",
    "model.add(layers.Bidirectional(layers.LSTM(10, activation='relu'), input_shape=(6,4)))\n",
    "\n",
    "# hidden layer\n",
    "model.add(layers.Dense(10, activation='relu'))\n",
    "\n",
    "# output layer\n",
    "model.add(layers.Dense(2))\n",
    "\n",
    "#compiling the model\n",
    "model.compile(optimizer='Adam', loss='mean_squared_error')\n",
    "\n",
    "# model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
