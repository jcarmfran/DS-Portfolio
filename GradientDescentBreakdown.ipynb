{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1\n",
    "\n",
    "- Receives an unknown array of numbers as input\n",
    "- Takes one step at a time toward the right and returns, in reverse order, the following:\n",
    "    - The smallest value before the values increase\n",
    "    - The index at which the value above was found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step_by_step_minimum(array):\n",
    "    length = len(array)\n",
    "    for index in range(length - 1):\n",
    "        if array[index] < array[index+1]:\n",
    "            return index, array[index]\n",
    "    return length - 1, array[length - 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 0)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "array = [8,7,6,4,2,0,1,2,3,4]\n",
    "test = step_by_step_minimum(array)\n",
    "#should return smallest value and index where value was found\n",
    "test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2\n",
    "- Takes an array and a positive index as inputs\n",
    "- Finds the smallest value by moving downwards in one direction\n",
    "    - Returns only this value\n",
    "    - If both directions move downward, it should move in the lowest direction\n",
    "- Iterates towards the minimum value, not through the whole array\n",
    "- Doesn't use the min() function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step_by_step_random(array, index):\n",
    "    fetch_minimum = lambda x: step_by_step_minimum(x)[1]\n",
    "    length = len(array)\n",
    "    if index == 0:\n",
    "        return fetch_minimum(array)\n",
    "    elif index == length - 1:\n",
    "        return fetch_minimum(array[::-1])\n",
    "    elif array[index-1] < array[index+1]:\n",
    "        return fetch_minimum(array[:index+1][::-1])\n",
    "    return fetch_minimum(array[index:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "array = [0,1,2,3,4,5,6,7,8,9,8,7,6,5,4]\n",
    "#starting off in 6th position of array\n",
    "test = step_by_step_random(array,5)\n",
    "#should return lowest value in array\n",
    "test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Univariate Gradient Descent Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def derivative(x):\n",
    "    return 2*(x-1)\n",
    "\n",
    "def update(x, alpha):\n",
    "    return x-alpha*derivative(x)\n",
    "\n",
    "def gradient_descent(x_0, alpha, iter_):\n",
    "    values = [x_0]\n",
    "    x = x_0\n",
    "    for n in range(iter_):\n",
    "        x = update(x, alpha)\n",
    "        values.append(x)\n",
    "    return values\n",
    "lr=2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stopping Criteria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return pow(x-1, 2)\n",
    "\n",
    "def derivative(x):\n",
    "    return 2*(x - 1)\n",
    "\n",
    "def update(x, alpha):\n",
    "    return x - alpha*derivative(x)\n",
    "\n",
    "def gradient_descent(x_0, alpha, tolerance, max_iter):\n",
    "    x = x_0\n",
    "    \n",
    "    for n in range(max_iter):\n",
    "        previous_image = f(x)\n",
    "        x = update(x, alpha)\n",
    "        current_image = f(x)\n",
    "        if abs(previous_image-current_image)< tolerance:\n",
    "            break\n",
    "    return x, f(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_squared_error(y_true, y_predicted):\n",
    "    cost = sum((y_true-y_predicted)**2) / len(y_true)\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(x, y, iterations = 1000, learning_rate = 0.0001, tolerance = 1e-4, current_weight = 0.1, current_bias = 0.01):\n",
    "    \n",
    "    current_weight = 0.1\n",
    "    current_bias = 0.01\n",
    "    n = float(len(x))\n",
    "    \n",
    "    costs = []\n",
    "    weights = []\n",
    "    previous_cost = None\n",
    "    for i in range(iterations): \n",
    "        y_predicted = x * current_weight + current_bias\n",
    "        current_cost = mean_squared_error(y, y_predicted)\n",
    "        \n",
    "        if previous_cost and abs(previous_cost-current_cost) < tolerance:\n",
    "            break\n",
    "        previous_cost = current_cost\n",
    "        costs.append(current_cost)\n",
    "        weights.append(current_weight)\n",
    "        \n",
    "        weight_derivative = -(2/n)*sum(x*(y-y_predicted))\n",
    "        bias_derivative = -(2/n)*sum(y-y_predicted)\n",
    "        \n",
    "        current_weight = current_weight - (learning_rate*weight_derivative)\n",
    "        current_bias = current_bias - (learning_rate*bias_derivative)\n",
    "    print(f\"Iteration {i+1}: Cost {current_cost}\")\n",
    "        \n",
    "    return current_weight, current_bias\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = pd.Series(data=[32.50234527, 53.42680403, 61.53035803, 47.47563963, 59.81320787, 55.14218841, 52.21179669, 39.29956669, 48.10504169, 52.55001444, 45.41973014, 54.35163488, 44.1640495 , 58.16847072, 56.72720806, 48.95588857, 44.68719623, 60.29732685, 45.61864377, 38.81681754])\n",
    "\n",
    "y = pd.Series(data=[31.70700585, 68.77759598, 62.5623823 , 71.54663223, 87.23092513, 78.21151827, 79.64197305, 59.17148932, 75.3312423 , 71.30087989, 55.16567715, 82.47884676, 62.00892325, 75.39287043, 81.43619216, 60.72360244, 82.89250373, 97.37989686, 48.84715332, 56.87721319])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_gd(X,Y, iterations):\n",
    "    estimated_weight, estimated_bias = gradient_descent(X, Y, iterations)\n",
    "    Y_pred = estimated_weight*X + estimated_bias\n",
    "    plt.figure(figsize = (8,6))\n",
    "    plt.scatter(X, Y, marker='o', color='red')\n",
    "    plt.plot(\n",
    "        [min(X), max(X)],\n",
    "        [min(Y_pred), max(Y_pred)],\n",
    "        color='blue', markerfacecolor='red',\n",
    "        markersize=10,linestyle='dashed'\n",
    "    )\n",
    "    plt.xlabel(\"X\")\n",
    "    plt.ylabel(\"Y\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in range(1, 10):\n",
    "    plot_gd(x, y, n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Breaking Down Stochastic Gradient Descent\n",
    "\n",
    "A less computationally expensive version of gradient descent algorithm\n",
    "\n",
    "When weight and bias are updated, we only use a sample of the data instead of all the data. In our case, we can take samples of `x`, `y`, and `y_predicted`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "mean = np.array([5.0, 7.0])\n",
    "cov = np.array([[1.0, 0.95], [0.95, 1.2]])\n",
    "data = np.random.multivariate_normal(mean, cov, 8000)\n",
    "\n",
    "X = np.array([n[0] for n in data]).reshape(-1,1)\n",
    "y = np.array([n[1] for n in data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (8,6))\n",
    "plt.scatter(X, y, marker = '.', color=\"red\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "sgdr = SGDRegressor() #instantiating the model\n",
    "sgdr.fit(X_train, y_train) #training the model\n",
    "y_pred = sgdr.predict(X_test) #making a prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mean_squared_error(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "### Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(x, y, init=np.array([0,0]), iterations=1000, learning_rate=0.0001, stopping_threshold=1e-6):\n",
    "# x: an array of values representing the predictors that will be used in the logistic regression\n",
    "# y: an array of values representing the outcomes associated with the predictors\n",
    "# init: an array of length 2 representing the initial guesses for the parameters — set the default to np.array([0, 0])\n",
    "# iterations: a number representing the maximum number of iterations gradient descent should take — set the default to 1000\n",
    "# learning_rate: a number representing the step size of the algorithm — set the default to 0.0001\n",
    "# stopping_threshold: a number representing the maximum difference between the estimated parameters — set the default to 1e-6\n",
    "\n",
    "    #set the previous cost and parameters\n",
    "    previous_cost = None\n",
    "    beta0 = init[0]\n",
    "    beta1 = init[1]\n",
    "    \n",
    "    #perform the gradient descent\n",
    "    for i in range(iterations):\n",
    "        \n",
    "        #calculate the predicted outcome based on the predictor\n",
    "        hz= 1/(1+np.exp(-1 * (beta0 + beta1 * x)))\n",
    "        \n",
    "        #calculate the log-loss cost based on current parameters\n",
    "        costs = y * (-np.log(hz)) + (1-y) * (-np.log(1-hz))\n",
    "        current_cost = sum(costs)\n",
    "        \n",
    "        # Check if we've met the conditions for breaking the loop\n",
    "        if previous_cost and abs(previous_cost-current_cost) <= stopping_threshold:\n",
    "            break # and return the parameters that minimize the log-loss cost (last line in function)\n",
    "        \n",
    "        previous_cost = current_cost\n",
    "        \n",
    "        beta0_derivative = np.mean(hz-y)\n",
    "        beta1_derivative = np.mean(x * (hz - y))\n",
    "        \n",
    "        beta0 = beta0 - learning_rate*beta0_derivative\n",
    "        beta1 = beta1 - learning_rate*beta1_derivative\n",
    "        \n",
    "    return np.array([beta0, beta1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_3_10_8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
